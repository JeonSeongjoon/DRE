{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfkUnMTTCkrf",
        "outputId": "9e265cc5-8df5-41e5-f2e5-dbe2fdfc5b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DRE'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 139 (delta 75), reused 69 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (139/139), 14.86 MiB | 42.40 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b branch_session https://github.com/JeonSeongjoon/DRE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9SYA1gXItB0",
        "outputId": "7325ec7d-2dd1-431c-adff-c82f31e704e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DRE\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DoCA1-3boM7",
        "outputId": "2d1591c9-562c-4995-df90-ca4853ba360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/376.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/376.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets peft trl bitsandbytes accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XnCsDjMJiS9"
      },
      "outputs": [],
      "source": [
        "!mkdir LoRA\n",
        "!mkdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "07Een3OgInYH",
        "outputId": "af41499c-d99e-46c6-ff13-12bd22318420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-16 23:14:31.022872: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-16 23:14:31.038135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752707671.056133    2701 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752707671.061379    2701 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-16 23:14:31.079301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 710/710 [00:00<00:00, 6.04MB/s]\n",
            "model.safetensors.index.json: 23.9kB [00:00, 84.2MB/s]\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   0% 551k/1.17G [00:00<29:30, 659kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 782k/4.98G [00:01<2:13:32, 621kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 840k/4.92G [00:01<2:11:45, 622kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 783k/5.00G [00:01<2:40:08, 520kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 2.52M/4.98G [00:01<54:23, 1.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 67.8M/5.00G [00:01<01:50, 44.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   3% 135M/5.00G [00:02<00:49, 97.9MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   6% 67.6M/1.17G [00:02<00:32, 34.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  12% 135M/1.17G [00:02<00:14, 71.6MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 271M/5.00G [00:02<00:25, 183MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  23% 269M/1.17G [00:02<00:05, 172MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 338M/5.00G [00:02<00:25, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 3.31M/4.92G [00:02<1:05:20, 1.25MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 405M/5.00G [00:03<00:25, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  29% 336M/1.17G [00:04<00:09, 92.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 4.95M/4.92G [00:04<1:03:07, 1.30MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   1% 71.9M/4.92G [00:04<02:49, 28.5MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 472M/5.00G [00:04<00:44, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  34% 402M/1.17G [00:07<00:18, 42.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 539M/5.00G [00:07<01:31, 48.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  40% 470M/1.17G [00:08<00:13, 51.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 673M/5.00G [00:08<01:00, 71.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   3% 139M/4.92G [00:08<04:02, 19.7MB/s] \u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  46% 537M/1.17G [00:10<00:13, 46.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  16% 807M/5.00G [00:10<00:55, 75.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 565M/1.17G [00:12<00:17, 34.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 206M/4.92G [00:12<03:58, 19.7MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 632M/1.17G [00:12<00:10, 49.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  60% 699M/1.17G [00:12<00:06, 67.6MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 273M/4.92G [00:12<02:34, 30.0MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 766M/1.17G [00:12<00:04, 86.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  71% 833M/1.17G [00:13<00:03, 102MB/s] \u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 340M/4.92G [00:13<01:58, 38.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 874M/5.00G [00:13<01:28, 46.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 900M/1.17G [00:13<00:02, 95.1MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:   8% 407M/4.92G [00:14<01:51, 40.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 69.6M/4.98G [00:15<16:58, 4.82MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   4% 204M/4.98G [00:16<04:50, 16.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 967M/1.17G [00:16<00:03, 52.0MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 474M/4.92G [00:16<01:52, 39.5MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 1.03G/1.17G [00:16<00:01, 68.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   5% 271M/4.98G [00:17<03:42, 21.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   7% 338M/4.98G [00:18<02:48, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 608M/4.92G [00:18<01:33, 46.2MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 1.10G/1.17G [00:18<00:01, 50.5MB/s]\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 743M/4.92G [00:19<00:58, 71.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 943M/5.00G [00:19<02:34, 26.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:   8% 407M/4.98G [00:20<02:26, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  10% 474M/4.98G [00:23<02:42, 27.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  12% 608M/4.98G [00:24<01:36, 45.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 1.01G/5.00G [00:24<03:08, 21.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  16% 810M/4.98G [00:24<00:51, 80.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 813M/4.92G [00:24<02:00, 34.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 1.08G/5.00G [00:25<02:23, 27.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  18% 877M/4.98G [00:25<00:47, 86.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:25<01:48, 35.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [00:28<01:02, 63.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 1.21G/5.00G [00:28<02:05, 30.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  22% 1.08G/4.98G [00:28<00:51, 75.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 880M/4.92G [00:28<02:22, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.35G/5.00G [00:29<01:17, 47.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [00:29<01:01, 57.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  19% 947M/4.92G [00:29<02:01, 32.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.48G/5.00G [00:32<01:21, 43.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:32<01:28, 43.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  21% 1.01G/4.92G [00:32<02:09, 30.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.55G/5.00G [00:32<01:00, 56.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.35G/4.98G [00:32<00:44, 81.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.61G/5.00G [00:32<00:44, 76.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.42G/4.98G [00:32<00:37, 95.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 1.15G/4.92G [00:33<01:27, 43.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:36<01:24, 39.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [00:36<01:09, 50.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  25% 1.22G/4.92G [00:36<01:41, 36.6MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:36<00:00, 32.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.82G/5.00G [00:36<00:46, 68.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.55G/4.98G [00:36<00:54, 63.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 1.35G/4.92G [00:36<01:00, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.88G/5.00G [00:37<00:46, 67.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  29% 1.42G/4.92G [00:37<00:57, 60.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  32% 1.62G/4.98G [00:37<00:53, 63.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [00:37<00:36, 83.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.48G/4.92G [00:40<01:18, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  34% 1.68G/4.92G [00:40<00:37, 86.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [00:40<01:17, 42.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  36% 1.75G/4.92G [00:40<00:30, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.82G/4.92G [00:41<00:27, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [00:41<01:02, 51.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  38% 1.89G/4.92G [00:42<00:29, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 2.02G/5.00G [00:42<01:23, 35.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  37% 1.82G/4.98G [00:42<01:05, 47.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [00:43<00:32, 90.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 2.15G/5.00G [00:43<00:56, 50.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  38% 1.89G/4.98G [00:43<00:58, 53.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  41% 2.02G/4.92G [00:44<00:33, 85.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  42% 2.09G/4.92G [00:45<00:36, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 2.22G/5.00G [00:45<00:53, 52.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [00:45<00:55, 54.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 2.28G/5.00G [00:45<00:44, 61.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  44% 2.15G/4.92G [00:45<00:35, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 2.35G/5.00G [00:46<00:39, 67.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  45% 2.22G/4.92G [00:46<00:33, 80.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  41% 2.02G/4.98G [00:46<01:03, 46.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 2.29G/4.92G [00:47<00:32, 81.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  42% 2.09G/4.98G [00:47<00:51, 56.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  48% 2.35G/4.92G [00:48<00:29, 86.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [00:48<00:43, 65.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 2.42G/4.92G [00:48<00:27, 89.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 2.22G/4.98G [00:49<00:39, 69.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  51% 2.49G/4.92G [00:49<00:27, 89.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  46% 2.29G/4.98G [00:49<00:36, 74.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  52% 2.55G/4.92G [00:50<00:32, 72.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:51<01:18, 32.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  53% 2.62G/4.92G [00:51<00:26, 87.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 2.36G/4.98G [00:51<00:43, 60.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 2.42G/4.98G [00:51<00:31, 80.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  55% 2.69G/4.92G [00:51<00:23, 96.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 2.49G/5.00G [00:52<01:09, 36.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  56% 2.76G/4.92G [00:52<00:24, 88.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [00:54<00:31, 67.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  50% 2.49G/4.98G [00:54<00:53, 46.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [00:54<01:09, 35.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.56G/4.98G [00:54<00:40, 60.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  59% 2.89G/4.92G [00:54<00:26, 77.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.63G/4.98G [00:55<00:29, 79.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  60% 2.96G/4.92G [00:55<00:20, 97.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 3.02G/4.92G [00:55<00:14, 127MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 3.16G/4.92G [00:55<00:08, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.70G/4.98G [00:55<00:23, 95.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 3.22G/4.92G [00:55<00:08, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.62G/5.00G [00:56<01:07, 35.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  67% 3.29G/4.92G [00:56<00:10, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.69G/5.00G [00:57<00:57, 40.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 3.36G/4.92G [00:57<00:14, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.75G/5.00G [00:58<00:46, 48.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.77G/4.98G [00:58<00:47, 46.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  70% 3.44G/4.92G [00:58<00:15, 94.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.82G/5.00G [00:59<00:40, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [00:59<00:37, 56.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 3.51G/4.92G [00:59<00:16, 87.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [00:59<00:35, 60.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.90G/4.98G [01:00<00:32, 63.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  73% 3.58G/4.92G [01:00<00:14, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.95G/5.00G [01:00<00:27, 73.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [01:00<00:21, 91.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 2.97G/4.98G [01:01<00:34, 58.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 3.65G/4.92G [01:01<00:18, 69.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  76% 3.71G/4.92G [01:02<00:15, 79.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  61% 3.03G/4.98G [01:02<00:34, 56.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 3.78G/4.92G [01:03<00:14, 79.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 3.10G/4.98G [01:04<00:39, 47.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.85G/4.92G [01:04<00:17, 61.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  62% 3.09G/5.00G [01:05<00:55, 34.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  64% 3.17G/4.98G [01:05<00:35, 51.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  80% 3.91G/4.92G [01:05<00:15, 62.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 3.16G/5.00G [01:06<00:41, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 3.24G/4.98G [01:06<00:30, 57.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  81% 3.98G/4.92G [01:07<00:14, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 3.22G/5.00G [01:07<00:38, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  66% 3.30G/4.98G [01:07<00:30, 54.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 4.05G/4.92G [01:08<00:13, 62.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 3.29G/5.00G [01:08<00:34, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 3.37G/4.98G [01:09<00:30, 53.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 3.33G/5.00G [01:09<00:35, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 4.11G/4.92G [01:09<00:14, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [01:10<00:26, 58.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  68% 3.39G/5.00G [01:10<00:32, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  85% 4.18G/4.92G [01:11<00:14, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 3.50G/4.98G [01:11<00:27, 54.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 3.57G/4.98G [01:12<00:22, 63.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 3.46G/5.00G [01:12<00:34, 45.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 4.25G/4.92G [01:12<00:13, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 3.64G/4.98G [01:12<00:18, 72.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 3.53G/5.00G [01:12<00:27, 54.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 4.31G/4.92G [01:13<00:09, 61.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 3.59G/5.00G [01:14<00:25, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  89% 4.38G/4.92G [01:14<00:09, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  76% 3.77G/4.98G [01:15<00:18, 66.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 3.66G/5.00G [01:15<00:23, 57.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 4.45G/4.92G [01:15<00:07, 59.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.84G/4.98G [01:15<00:14, 77.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.91G/4.98G [01:15<00:12, 88.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 3.73G/5.00G [01:16<00:21, 60.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 4.51G/4.92G [01:16<00:06, 63.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.97G/4.98G [01:16<00:12, 80.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 3.79G/5.00G [01:17<00:20, 58.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  93% 4.58G/4.92G [01:17<00:05, 62.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 4.04G/4.98G [01:17<00:12, 77.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 3.86G/5.00G [01:18<00:18, 63.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 4.11G/4.98G [01:18<00:11, 78.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.93G/5.00G [01:19<00:16, 67.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  84% 4.17G/4.98G [01:19<00:09, 80.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  80% 3.99G/5.00G [01:20<00:15, 66.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  95% 4.65G/4.92G [01:20<00:06, 41.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 4.24G/4.98G [01:21<00:12, 58.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 4.06G/5.00G [01:21<00:16, 57.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 4.13G/5.00G [01:22<00:13, 66.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 4.31G/4.98G [01:22<00:10, 61.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 4.71G/4.92G [01:22<00:05, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  84% 4.19G/5.00G [01:23<00:11, 68.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  88% 4.37G/4.98G [01:23<00:09, 64.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  85% 4.26G/5.00G [01:23<00:10, 72.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  89% 4.44G/4.98G [01:23<00:07, 69.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  87% 4.33G/5.00G [01:24<00:07, 90.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 4.78G/4.92G [01:24<00:03, 37.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [01:24<00:05, 85.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors:  99% 4.85G/4.92G [01:24<00:01, 46.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  88% 4.40G/5.00G [01:25<00:06, 87.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  89% 4.46G/5.00G [01:26<00:06, 83.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 4.57G/4.98G [01:26<00:06, 62.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [01:26<00:00, 56.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 4.53G/5.00G [01:26<00:05, 81.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  93% 4.64G/4.98G [01:26<00:04, 67.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  95% 4.71G/4.98G [01:27<00:03, 73.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  92% 4.60G/5.00G [01:28<00:05, 72.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 4.78G/4.98G [01:28<00:02, 87.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  99% 4.91G/4.98G [01:28<00:00, 133MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  93% 4.66G/5.00G [01:28<00:04, 81.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [01:28<00:00, 56.1MB/s]\n",
            "Fetching 4 files:  25% 1/4 [01:28<04:26, 88.88s/it]\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 4.73G/5.00G [01:28<00:02, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [01:29<00:01, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [01:29<00:00, 156MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  99% 4.93G/5.00G [01:29<00:00, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [01:29<00:00, 55.7MB/s]\n",
            "Fetching 4 files: 100% 4/4 [01:29<00:00, 22.49s/it]\n",
            "Loading checkpoint shards: 100% 4/4 [00:17<00:00,  4.41s/it]\n",
            "generation_config.json: 100% 172/172 [00:00<00:00, 1.82MB/s]\n",
            "tokenizer_config.json: 51.1kB [00:00, 147MB/s]\n",
            "tokenizer.json: 9.09MB [00:00, 85.8MB/s]\n",
            "special_tokens_map.json: 100% 444/444 [00:00<00:00, 5.31MB/s]\n",
            "Map:   0% 0/3000 [00:00<?, ? examples/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 3000/3000 [00:03<00:00, 894.22 examples/s]\n",
            "Truncating train dataset: 100% 3000/3000 [00:00<00:00, 38212.13 examples/s]\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "  0% 0/1125 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 5.452, 'grad_norm': 27.023176193237305, 'learning_rate': 2.9411764705882354e-05, 'num_tokens': 81920.0, 'mean_token_accuracy': 0.06077712585683912, 'epoch': 0.03}\n",
            "{'loss': 2.8682, 'grad_norm': 1.0404795408248901, 'learning_rate': 8.823529411764706e-05, 'num_tokens': 163840.0, 'mean_token_accuracy': 0.6246700854622759, 'epoch': 0.05}\n",
            "{'loss': 1.6568, 'grad_norm': 1.616815209388733, 'learning_rate': 0.00014705882352941178, 'num_tokens': 245760.0, 'mean_token_accuracy': 0.8912267744541168, 'epoch': 0.08}\n",
            "{'loss': 0.972, 'grad_norm': 0.5465003252029419, 'learning_rate': 0.00019999958540892524, 'num_tokens': 327680.0, 'mean_token_accuracy': 0.9000366523861885, 'epoch': 0.11}\n",
            "{'loss': 0.7352, 'grad_norm': 0.6656064391136169, 'learning_rate': 0.00019994983863945388, 'num_tokens': 409600.0, 'mean_token_accuracy': 0.9161534771323204, 'epoch': 0.13}\n",
            "  4% 50/1125 [08:21<2:58:22,  9.96s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.6706, 'grad_norm': 0.5356206893920898, 'learning_rate': 0.00019981722091716783, 'num_tokens': 491520.0, 'mean_token_accuracy': 0.9180474206805229, 'epoch': 0.16}\n",
            "{'loss': 0.6549, 'grad_norm': 0.795359194278717, 'learning_rate': 0.00019960184219879303, 'num_tokens': 573440.0, 'mean_token_accuracy': 0.9183773308992386, 'epoch': 0.19}\n",
            "{'loss': 0.6458, 'grad_norm': 0.6502760052680969, 'learning_rate': 0.00019930388106030166, 'num_tokens': 655360.0, 'mean_token_accuracy': 0.9183284573256969, 'epoch': 0.21}\n",
            "{'loss': 0.6441, 'grad_norm': 0.28993913531303406, 'learning_rate': 0.00019892358454885042, 'num_tokens': 737280.0, 'mean_token_accuracy': 0.917167653888464, 'epoch': 0.24}\n",
            "{'loss': 0.6143, 'grad_norm': 0.5211877226829529, 'learning_rate': 0.00019846126797794743, 'num_tokens': 819200.0, 'mean_token_accuracy': 0.9203201472759247, 'epoch': 0.27}\n",
            "  9% 100/1125 [16:40<2:50:21,  9.97s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.6386, 'grad_norm': 0.16877755522727966, 'learning_rate': 0.00019791731466601773, 'num_tokens': 901120.0, 'mean_token_accuracy': 0.9172531858086586, 'epoch': 0.29}\n",
            "{'loss': 0.6203, 'grad_norm': 0.20563413202762604, 'learning_rate': 0.00019729217561858433, 'num_tokens': 983040.0, 'mean_token_accuracy': 0.918279580026865, 'epoch': 0.32}\n",
            "{'loss': 0.5961, 'grad_norm': 0.25239020586013794, 'learning_rate': 0.00019658636915432788, 'num_tokens': 1064960.0, 'mean_token_accuracy': 0.9212610080838204, 'epoch': 0.35}\n",
            "{'loss': 0.6277, 'grad_norm': 0.13967682421207428, 'learning_rate': 0.00019580048047533578, 'num_tokens': 1146880.0, 'mean_token_accuracy': 0.9167522072792054, 'epoch': 0.37}\n",
            "{'loss': 0.6431, 'grad_norm': 0.2761232554912567, 'learning_rate': 0.00019493516118189582, 'num_tokens': 1228800.0, 'mean_token_accuracy': 0.9148704811930657, 'epoch': 0.4}\n",
            " 13% 150/1125 [25:00<2:42:03,  9.97s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.6096, 'grad_norm': 0.15470834076404572, 'learning_rate': 0.00019399112873223824, 'num_tokens': 1310720.0, 'mean_token_accuracy': 0.9198558248579503, 'epoch': 0.43}\n",
            "{'loss': 0.6002, 'grad_norm': 0.16764096915721893, 'learning_rate': 0.00019296916584767262, 'num_tokens': 1392640.0, 'mean_token_accuracy': 0.9197702944278717, 'epoch': 0.45}\n",
            "{'loss': 0.6126, 'grad_norm': 0.20540288090705872, 'learning_rate': 0.00019187011986361374, 'num_tokens': 1474560.0, 'mean_token_accuracy': 0.9183773323893547, 'epoch': 0.48}\n",
            "{'loss': 0.6027, 'grad_norm': 0.23078389465808868, 'learning_rate': 0.00019069490202703438, 'num_tokens': 1556480.0, 'mean_token_accuracy': 0.9190860293805599, 'epoch': 0.51}\n",
            "{'loss': 0.5975, 'grad_norm': 0.3521634340286255, 'learning_rate': 0.00018944448674092714, 'num_tokens': 1638400.0, 'mean_token_accuracy': 0.9206011824309825, 'epoch': 0.53}\n",
            " 18% 200/1125 [33:19<2:33:49,  9.98s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5998, 'grad_norm': 0.16527904570102692, 'learning_rate': 0.00018811991075640223, 'num_tokens': 1720320.0, 'mean_token_accuracy': 0.9196969799697399, 'epoch': 0.56}\n",
            "{'loss': 0.5996, 'grad_norm': 0.12850292026996613, 'learning_rate': 0.00018672227231309068, 'num_tokens': 1802240.0, 'mean_token_accuracy': 0.9198802627623082, 'epoch': 0.59}\n",
            "{'loss': 0.5916, 'grad_norm': 0.14444521069526672, 'learning_rate': 0.00018525273022856607, 'num_tokens': 1884160.0, 'mean_token_accuracy': 0.920197955518961, 'epoch': 0.61}\n",
            "{'loss': 0.6032, 'grad_norm': 0.16384190320968628, 'learning_rate': 0.0001837125029375393, 'num_tokens': 1966080.0, 'mean_token_accuracy': 0.9193059712648392, 'epoch': 0.64}\n",
            "{'loss': 0.5898, 'grad_norm': 0.22264769673347473, 'learning_rate': 0.00018210286748162336, 'num_tokens': 2048000.0, 'mean_token_accuracy': 0.9201857395470142, 'epoch': 0.67}\n",
            " 22% 250/1125 [41:40<2:25:31,  9.98s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5931, 'grad_norm': 0.2651640474796295, 'learning_rate': 0.00018042515845050576, 'num_tokens': 2129920.0, 'mean_token_accuracy': 0.9195625714957714, 'epoch': 0.69}\n",
            "{'loss': 0.5944, 'grad_norm': 0.2521684467792511, 'learning_rate': 0.00017868076687540624, 'num_tokens': 2211840.0, 'mean_token_accuracy': 0.9192815348505974, 'epoch': 0.72}\n",
            "{'loss': 0.5889, 'grad_norm': 0.22166796028614044, 'learning_rate': 0.0001768711390757374, 'num_tokens': 2293760.0, 'mean_token_accuracy': 0.9202957101166248, 'epoch': 0.75}\n",
            "{'loss': 0.5934, 'grad_norm': 0.14905871450901031, 'learning_rate': 0.00017499777545992452, 'num_tokens': 2375680.0, 'mean_token_accuracy': 0.9198924817144871, 'epoch': 0.77}\n",
            "{'loss': 0.6088, 'grad_norm': 0.15023386478424072, 'learning_rate': 0.00017306222928137875, 'num_tokens': 2457600.0, 'mean_token_accuracy': 0.9179374493658543, 'epoch': 0.8}\n",
            " 27% 300/1125 [49:59<2:17:37, 10.01s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5766, 'grad_norm': 0.13695044815540314, 'learning_rate': 0.00017106610535065517, 'num_tokens': 2539520.0, 'mean_token_accuracy': 0.9212365709245205, 'epoch': 0.83}\n",
            "{'loss': 0.5928, 'grad_norm': 0.14618101716041565, 'learning_rate': 0.00016901105870486372, 'num_tokens': 2621440.0, 'mean_token_accuracy': 0.9189394034445286, 'epoch': 0.85}\n",
            "{'loss': 0.6017, 'grad_norm': 0.22488048672676086, 'learning_rate': 0.00016689879323543566, 'num_tokens': 2703360.0, 'mean_token_accuracy': 0.9192570924758912, 'epoch': 0.88}\n",
            "{'loss': 0.5986, 'grad_norm': 0.14586138725280762, 'learning_rate': 0.00016473106027538393, 'num_tokens': 2785280.0, 'mean_token_accuracy': 0.9196969792246819, 'epoch': 0.91}\n",
            "{'loss': 0.6041, 'grad_norm': 0.16523802280426025, 'learning_rate': 0.0001625096571472285, 'num_tokens': 2867200.0, 'mean_token_accuracy': 0.9180474169552326, 'epoch': 0.93}\n",
            " 31% 350/1125 [58:19<2:08:39,  9.96s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.6051, 'grad_norm': 0.19210360944271088, 'learning_rate': 0.00016023642567279033, 'num_tokens': 2949120.0, 'mean_token_accuracy': 0.9178152561187745, 'epoch': 0.96}\n",
            "{'loss': 0.5821, 'grad_norm': 0.17921452224254608, 'learning_rate': 0.0001579132506460903, 'num_tokens': 3031040.0, 'mean_token_accuracy': 0.9205523081123829, 'epoch': 0.99}\n",
            "{'loss': 0.5727, 'grad_norm': 0.1573258489370346, 'learning_rate': 0.00015554205827061855, 'num_tokens': 3112960.0, 'mean_token_accuracy': 0.9211876936256885, 'epoch': 1.01}\n",
            "{'loss': 0.5878, 'grad_norm': 0.13947749137878418, 'learning_rate': 0.00015312481456226986, 'num_tokens': 3194880.0, 'mean_token_accuracy': 0.9199047036468982, 'epoch': 1.04}\n",
            "{'loss': 0.5929, 'grad_norm': 0.21561573445796967, 'learning_rate': 0.00015066352371927047, 'num_tokens': 3276800.0, 'mean_token_accuracy': 0.9187316820025444, 'epoch': 1.07}\n",
            " 36% 400/1125 [1:06:38<2:00:24,  9.96s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5992, 'grad_norm': 0.1481999009847641, 'learning_rate': 0.0001481602264604457, 'num_tokens': 3358720.0, 'mean_token_accuracy': 0.9179374448955059, 'epoch': 1.09}\n",
            "{'loss': 0.6267, 'grad_norm': 0.2461112141609192, 'learning_rate': 0.0001456169983332087, 'num_tokens': 3440640.0, 'mean_token_accuracy': 0.9143450707197189, 'epoch': 1.12}\n",
            "{'loss': 0.5829, 'grad_norm': 0.17727340757846832, 'learning_rate': 0.00014303594799267065, 'num_tokens': 3522560.0, 'mean_token_accuracy': 0.9199535772204399, 'epoch': 1.15}\n",
            "{'loss': 0.5997, 'grad_norm': 0.2886855900287628, 'learning_rate': 0.00014041921545330193, 'num_tokens': 3604480.0, 'mean_token_accuracy': 0.9185483969748021, 'epoch': 1.17}\n",
            "{'loss': 0.5874, 'grad_norm': 0.14779910445213318, 'learning_rate': 0.00013776897031459104, 'num_tokens': 3686400.0, 'mean_token_accuracy': 0.9191715635359288, 'epoch': 1.2}\n",
            " 40% 450/1125 [1:14:58<1:52:26,  9.99s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5647, 'grad_norm': 0.38239696621894836, 'learning_rate': 0.00013508740996217493, 'num_tokens': 3768320.0, 'mean_token_accuracy': 0.9230083175003528, 'epoch': 1.23}\n",
            "{'loss': 0.5982, 'grad_norm': 0.12465041130781174, 'learning_rate': 0.00013237675774593045, 'num_tokens': 3850240.0, 'mean_token_accuracy': 0.9184628620743751, 'epoch': 1.25}\n",
            "{'loss': 0.5669, 'grad_norm': 0.1947270631790161, 'learning_rate': 0.00012963926113653863, 'num_tokens': 3932160.0, 'mean_token_accuracy': 0.9220185816287995, 'epoch': 1.28}\n",
            "{'loss': 0.5798, 'grad_norm': 0.2547735571861267, 'learning_rate': 0.0001268771898620494, 'num_tokens': 4014080.0, 'mean_token_accuracy': 0.9204056806862354, 'epoch': 1.31}\n",
            "{'loss': 0.5837, 'grad_norm': 0.1459021121263504, 'learning_rate': 0.00012409283402599238, 'num_tokens': 4096000.0, 'mean_token_accuracy': 0.9197947286069393, 'epoch': 1.33}\n",
            " 44% 500/1125 [1:23:17<1:43:52,  9.97s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5809, 'grad_norm': 0.25366663932800293, 'learning_rate': 0.00012128850220859397, 'num_tokens': 4177920.0, 'mean_token_accuracy': 0.9205645263195038, 'epoch': 1.36}\n",
            "{'loss': 0.599, 'grad_norm': 0.24247293174266815, 'learning_rate': 0.00011846651955267463, 'num_tokens': 4259840.0, 'mean_token_accuracy': 0.9174609087407589, 'epoch': 1.39}\n",
            "{'loss': 0.5786, 'grad_norm': 0.19006697833538055, 'learning_rate': 0.00011562922583581375, 'num_tokens': 4341760.0, 'mean_token_accuracy': 0.9197092011570931, 'epoch': 1.41}\n",
            "{'loss': 0.5863, 'grad_norm': 0.2357952892780304, 'learning_rate': 0.00011277897353038085, 'num_tokens': 4423680.0, 'mean_token_accuracy': 0.9194526009261608, 'epoch': 1.44}\n",
            "{'loss': 0.5861, 'grad_norm': 0.17940343916416168, 'learning_rate': 0.00010991812585304069, 'num_tokens': 4505600.0, 'mean_token_accuracy': 0.9194403804838658, 'epoch': 1.47}\n",
            " 49% 550/1125 [1:31:37<1:35:32,  9.97s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5901, 'grad_norm': 0.14401625096797943, 'learning_rate': 0.0001070490548053503, 'num_tokens': 4587520.0, 'mean_token_accuracy': 0.9185972712934017, 'epoch': 1.49}\n",
            "{'loss': 0.598, 'grad_norm': 0.1748504489660263, 'learning_rate': 0.00010417413920707222, 'num_tokens': 4669440.0, 'mean_token_accuracy': 0.9176319725811481, 'epoch': 1.52}\n",
            "{'loss': 0.5923, 'grad_norm': 0.21523763239383698, 'learning_rate': 0.00010129576272383445, 'num_tokens': 4751360.0, 'mean_token_accuracy': 0.9188172116875648, 'epoch': 1.55}\n",
            "{'loss': 0.5791, 'grad_norm': 0.11843199282884598, 'learning_rate': 9.841631189077269e-05, 'num_tokens': 4833280.0, 'mean_token_accuracy': 0.9200024552643299, 'epoch': 1.57}\n",
            "{'loss': 0.5744, 'grad_norm': 0.24830476939678192, 'learning_rate': 9.553817413379356e-05, 'num_tokens': 4915200.0, 'mean_token_accuracy': 0.9213954150676728, 'epoch': 1.6}\n",
            " 53% 600/1125 [1:39:57<1:27:30, 10.00s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.6048, 'grad_norm': 0.1600896418094635, 'learning_rate': 9.266373579009867e-05, 'num_tokens': 4997120.0, 'mean_token_accuracy': 0.9163367621600628, 'epoch': 1.63}\n",
            "{'loss': 0.5614, 'grad_norm': 0.1683652251958847, 'learning_rate': 8.979538012961221e-05, 'num_tokens': 5079040.0, 'mean_token_accuracy': 0.9222507424652576, 'epoch': 1.65}\n",
            "{'loss': 0.5799, 'grad_norm': 0.37044256925582886, 'learning_rate': 8.69354853789509e-05, 'num_tokens': 5160960.0, 'mean_token_accuracy': 0.9201124235987663, 'epoch': 1.68}\n",
            "{'loss': 0.5843, 'grad_norm': 0.17888574302196503, 'learning_rate': 8.408642274957612e-05, 'num_tokens': 5242880.0, 'mean_token_accuracy': 0.919978016614914, 'epoch': 1.71}\n",
            "{'loss': 0.58, 'grad_norm': 0.15266631543636322, 'learning_rate': 8.125055447176186e-05, 'num_tokens': 5324800.0, 'mean_token_accuracy': 0.9198558241128921, 'epoch': 1.73}\n",
            " 58% 650/1125 [1:48:18<1:19:02,  9.98s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5911, 'grad_norm': 0.17821715772151947, 'learning_rate': 7.843023183600988e-05, 'num_tokens': 5406720.0, 'mean_token_accuracy': 0.9179863229393959, 'epoch': 1.76}\n",
            "{'loss': 0.5777, 'grad_norm': 0.2511926591396332, 'learning_rate': 7.562779324353477e-05, 'num_tokens': 5488640.0, 'mean_token_accuracy': 0.9195136971771717, 'epoch': 1.79}\n",
            "{'loss': 0.5927, 'grad_norm': 0.15284162759780884, 'learning_rate': 7.284556226743598e-05, 'num_tokens': 5570560.0, 'mean_token_accuracy': 0.9177786022424698, 'epoch': 1.81}\n",
            "{'loss': 0.5816, 'grad_norm': 0.27981263399124146, 'learning_rate': 7.008584572616448e-05, 'num_tokens': 5652480.0, 'mean_token_accuracy': 0.9196969777345657, 'epoch': 1.84}\n",
            "{'loss': 0.5865, 'grad_norm': 0.23832781612873077, 'learning_rate': 6.73509317708807e-05, 'num_tokens': 5734400.0, 'mean_token_accuracy': 0.9189394004642963, 'epoch': 1.87}\n",
            " 62% 700/1125 [1:56:38<1:10:48, 10.00s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5665, 'grad_norm': 0.14448760449886322, 'learning_rate': 6.464308798829043e-05, 'num_tokens': 5816320.0, 'mean_token_accuracy': 0.9213954150676728, 'epoch': 1.89}\n",
            "{'loss': 0.5785, 'grad_norm': 0.14589384198188782, 'learning_rate': 6.196455952053084e-05, 'num_tokens': 5898240.0, 'mean_token_accuracy': 0.9195259124040603, 'epoch': 1.92}\n",
            "{'loss': 0.5823, 'grad_norm': 0.24281658232212067, 'learning_rate': 5.931756720366621e-05, 'num_tokens': 5980160.0, 'mean_token_accuracy': 0.9203934609889984, 'epoch': 1.95}\n",
            "{'loss': 0.5791, 'grad_norm': 0.14075306057929993, 'learning_rate': 5.670430572633607e-05, 'num_tokens': 6062080.0, 'mean_token_accuracy': 0.9195136941969395, 'epoch': 1.97}\n",
            "{'loss': 0.5901, 'grad_norm': 0.15540626645088196, 'learning_rate': 5.412694181008329e-05, 'num_tokens': 6144000.0, 'mean_token_accuracy': 0.9191715620458126, 'epoch': 2.0}\n",
            " 67% 750/1125 [2:04:59<1:02:26,  9.99s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5698, 'grad_norm': 0.1773746907711029, 'learning_rate': 5.1587612412869954e-05, 'num_tokens': 6225920.0, 'mean_token_accuracy': 0.9210044056177139, 'epoch': 2.03}\n",
            "{'loss': 0.5725, 'grad_norm': 0.3010241687297821, 'learning_rate': 4.9088422957271164e-05, 'num_tokens': 6307840.0, 'mean_token_accuracy': 0.9208822160959244, 'epoch': 2.05}\n",
            "{'loss': 0.5805, 'grad_norm': 0.2028304934501648, 'learning_rate': 4.6631445584815926e-05, 'num_tokens': 6389760.0, 'mean_token_accuracy': 0.9194525994360447, 'epoch': 2.08}\n",
            "{'loss': 0.5965, 'grad_norm': 0.29892706871032715, 'learning_rate': 4.4218717437921445e-05, 'num_tokens': 6471680.0, 'mean_token_accuracy': 0.9171065576374531, 'epoch': 2.11}\n",
            "{'loss': 0.5764, 'grad_norm': 0.31175071001052856, 'learning_rate': 4.185223897084709e-05, 'num_tokens': 6553600.0, 'mean_token_accuracy': 0.9196725398302078, 'epoch': 2.13}\n",
            " 71% 800/1125 [2:13:19<54:03,  9.98s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5889, 'grad_norm': 0.24020834267139435, 'learning_rate': 3.953397229106636e-05, 'num_tokens': 6635520.0, 'mean_token_accuracy': 0.9180962949991226, 'epoch': 2.16}\n",
            "{'loss': 0.5774, 'grad_norm': 0.1391238272190094, 'learning_rate': 3.7265839532434155e-05, 'num_tokens': 6717440.0, 'mean_token_accuracy': 0.919660323113203, 'epoch': 2.19}\n",
            "{'loss': 0.5981, 'grad_norm': 0.2350715845823288, 'learning_rate': 3.504972126149639e-05, 'num_tokens': 6799360.0, 'mean_token_accuracy': 0.9166666775941849, 'epoch': 2.21}\n",
            "{'loss': 0.5851, 'grad_norm': 0.16367767751216888, 'learning_rate': 3.288745491826459e-05, 'num_tokens': 6881280.0, 'mean_token_accuracy': 0.9187316812574864, 'epoch': 2.24}\n",
            "{'loss': 0.5745, 'grad_norm': 0.23452597856521606, 'learning_rate': 3.078083329274767e-05, 'num_tokens': 6963200.0, 'mean_token_accuracy': 0.9200757689774036, 'epoch': 2.27}\n",
            " 76% 850/1125 [2:21:39<45:52, 10.01s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5938, 'grad_norm': 0.1296674907207489, 'learning_rate': 2.873160303850405e-05, 'num_tokens': 7045120.0, 'mean_token_accuracy': 0.917399813234806, 'epoch': 2.29}\n",
            "{'loss': 0.5816, 'grad_norm': 0.15091922879219055, 'learning_rate': 2.6741463224446926e-05, 'num_tokens': 7127040.0, 'mean_token_accuracy': 0.9193792834877967, 'epoch': 2.32}\n",
            "{'loss': 0.5623, 'grad_norm': 0.15473279356956482, 'learning_rate': 2.481206392610278e-05, 'num_tokens': 7208960.0, 'mean_token_accuracy': 0.921322102099657, 'epoch': 2.35}\n",
            "{'loss': 0.5686, 'grad_norm': 0.3013145923614502, 'learning_rate': 2.294500485749218e-05, 'num_tokens': 7290880.0, 'mean_token_accuracy': 0.9211510382592678, 'epoch': 2.37}\n",
            "{'loss': 0.5797, 'grad_norm': 0.1999068409204483, 'learning_rate': 2.1141834044765774e-05, 'num_tokens': 7372800.0, 'mean_token_accuracy': 0.9189394041895866, 'epoch': 2.4}\n",
            " 80% 900/1125 [2:30:00<37:29, 10.00s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.573, 'grad_norm': 0.14913301169872284, 'learning_rate': 1.940404654269684e-05, 'num_tokens': 7454720.0, 'mean_token_accuracy': 0.9209310919046402, 'epoch': 2.43}\n",
            "{'loss': 0.5841, 'grad_norm': 0.23283429443836212, 'learning_rate': 1.7733083195093058e-05, 'num_tokens': 7536640.0, 'mean_token_accuracy': 0.9191593430936337, 'epoch': 2.45}\n",
            "{'loss': 0.5942, 'grad_norm': 0.18813550472259521, 'learning_rate': 1.6130329440156432e-05, 'num_tokens': 7618560.0, 'mean_token_accuracy': 0.9174975641071796, 'epoch': 2.48}\n",
            "{'loss': 0.5851, 'grad_norm': 0.22004729509353638, 'learning_rate': 1.4597114161781188e-05, 'num_tokens': 7700480.0, 'mean_token_accuracy': 0.9187316797673702, 'epoch': 2.51}\n",
            "{'loss': 0.5744, 'grad_norm': 0.18967974185943604, 'learning_rate': 1.3134708587742362e-05, 'num_tokens': 7782400.0, 'mean_token_accuracy': 0.9194892577826976, 'epoch': 2.53}\n",
            " 84% 950/1125 [2:38:20<29:10, 10.00s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5638, 'grad_norm': 0.1957092434167862, 'learning_rate': 1.1744325235688536e-05, 'num_tokens': 7864320.0, 'mean_token_accuracy': 0.9215298227965831, 'epoch': 2.56}\n",
            "{'loss': 0.5828, 'grad_norm': 0.32839062809944153, 'learning_rate': 1.042711690781254e-05, 'num_tokens': 7946240.0, 'mean_token_accuracy': 0.9192204393446446, 'epoch': 2.59}\n",
            "{'loss': 0.573, 'grad_norm': 0.3058546185493469, 'learning_rate': 9.18417573503404e-06, 'num_tokens': 8028160.0, 'mean_token_accuracy': 0.9199291408061981, 'epoch': 2.61}\n",
            "{'loss': 0.5781, 'grad_norm': 0.3369264006614685, 'learning_rate': 8.016532271485789e-06, 'num_tokens': 8110080.0, 'mean_token_accuracy': 0.9194159425795079, 'epoch': 2.64}\n",
            "{'loss': 0.5674, 'grad_norm': 0.16401201486587524, 'learning_rate': 6.92515464005511e-06, 'num_tokens': 8192000.0, 'mean_token_accuracy': 0.9204423367977143, 'epoch': 2.67}\n",
            " 89% 1000/1125 [2:46:41<20:52, 10.02s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5578, 'grad_norm': 0.17232444882392883, 'learning_rate': 5.910947729688598e-06, 'num_tokens': 8273920.0, 'mean_token_accuracy': 0.922458465397358, 'epoch': 2.69}\n",
            "{'loss': 0.5802, 'grad_norm': 0.21713930368423462, 'learning_rate': 4.97475244512563e-06, 'num_tokens': 8355840.0, 'mean_token_accuracy': 0.9192082203924656, 'epoch': 2.72}\n",
            "{'loss': 0.5595, 'grad_norm': 0.2678326368331909, 'learning_rate': 4.117345009682916e-06, 'num_tokens': 8437760.0, 'mean_token_accuracy': 0.9219941429793834, 'epoch': 2.75}\n",
            "{'loss': 0.5594, 'grad_norm': 0.16735194623470306, 'learning_rate': 3.3394363216679656e-06, 'num_tokens': 8519680.0, 'mean_token_accuracy': 0.9213587589561939, 'epoch': 2.77}\n",
            "{'loss': 0.6001, 'grad_norm': 0.2593907415866852, 'learning_rate': 2.641671364955256e-06, 'num_tokens': 8601600.0, 'mean_token_accuracy': 0.9170332446694374, 'epoch': 2.8}\n",
            " 93% 1050/1125 [2:55:03<12:42, 10.17s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5831, 'grad_norm': 0.17197100818157196, 'learning_rate': 2.0246286742136913e-06, 'num_tokens': 8683520.0, 'mean_token_accuracy': 0.9190004996955394, 'epoch': 2.83}\n",
            "{'loss': 0.5557, 'grad_norm': 0.18467259407043457, 'learning_rate': 1.4888198552287624e-06, 'num_tokens': 8765440.0, 'mean_token_accuracy': 0.9218597367405892, 'epoch': 2.85}\n",
            "{'loss': 0.5754, 'grad_norm': 0.31473007798194885, 'learning_rate': 1.0346891607172614e-06, 'num_tokens': 8847360.0, 'mean_token_accuracy': 0.919660321623087, 'epoch': 2.88}\n",
            "{'loss': 0.5883, 'grad_norm': 0.24096883833408356, 'learning_rate': 6.626131219859555e-07, 'num_tokens': 8929280.0, 'mean_token_accuracy': 0.9184750825166702, 'epoch': 2.91}\n",
            "{'loss': 0.5711, 'grad_norm': 0.19380182027816772, 'learning_rate': 3.729002367399925e-07, 'num_tokens': 9011200.0, 'mean_token_accuracy': 0.9208211220800877, 'epoch': 2.93}\n",
            " 98% 1100/1125 [3:03:24<04:09,  9.99s/it]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'loss': 0.5977, 'grad_norm': 0.18956410884857178, 'learning_rate': 1.6579071329957395e-07, 'num_tokens': 9093120.0, 'mean_token_accuracy': 0.9170698992908001, 'epoch': 2.96}\n",
            "{'loss': 0.5766, 'grad_norm': 0.2136484980583191, 'learning_rate': 4.14562714371014e-08, 'num_tokens': 9175040.0, 'mean_token_accuracy': 0.9197947300970555, 'epoch': 2.99}\n",
            "{'train_runtime': 11255.6477, 'train_samples_per_second': 0.8, 'train_steps_per_second': 0.1, 'train_loss': 0.6675859004126655, 'num_tokens': 9216000.0, 'mean_token_accuracy': 0.9199657931923866, 'epoch': 3.0}\n",
            "100% 1125/1125 [3:07:35<00:00, 10.01s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "\n",
            "파인튜닝 완료 후 테스트 :\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "#0\n",
            "input : \n",
            "A : How much are they?\n",
            "B : They are 3$ each.\n",
            "A : Too expansive... Any discount?\n",
            "B : No, they are already the cheapest in this town.\n",
            "A : Okay, then I'll take it.\n",
            "\n",
            "\n",
            "translation result : English: \n",
            "A : How much are they?\n",
            "B : They are 3$ each.\n",
            "A : Too expansive... Any discount?\n",
            "B : No, they are already the cheapest in this town.\n",
            "A : Okay, then I'll take it.\n",
            "\n",
            "\n",
            "\n",
            "Korean:  B:  B:  B:  B.  B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B.\n",
            "\n",
            "\n",
            "\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
            "#1\n",
            "input : \n",
            "B : Excuse me. Where is the history Museum?\n",
            "G : Its on the green street.\n",
            "B : How do I get there?\n",
            "G : Go straight three blocks and turn right.\n",
            "B : Thank you so much.\n",
            "\n",
            "\n",
            "translation result : English: \n",
            "B : Excuse me. Where is the history Museum?\n",
            "G : Its on the green street.\n",
            "B : How do I get there?\n",
            "G : Go straight three blocks and turn right.\n",
            "B : Thank you so much.\n",
            "\n",
            "\n",
            "\n",
            "Korean: ................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "\n",
            "\n",
            "\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
            "#2\n",
            "input : \n",
            "I: Professor, I'm so sorry, but I completely forgot about the exam date. Is there anything I can do?\n",
            "J: Well, you forgot? How did that happen?\n",
            "I: I was sick last week, and I just haven't been myself for the last couple of weeks.\n",
            "J: Hmm... OK, I'll give you another chance this time. You can either take a make-up exam or write a research paper.\n",
            "I: Thank you very much, professor. I'll take the make-up exam.\n",
            "\n",
            "\n",
            "translation result : English: \n",
            "I: Professor, I'm so sorry, but I completely forgot about the exam date. Is there anything I can do?\n",
            "J: Well, you forgot? How did that happen?\n",
            "I: I was sick last week, and I just haven't been myself for the last couple of weeks.\n",
            "J: Hmm... OK, I'll give you another chance this time. You can either take a make-up exam or write a research paper.\n",
            "I: Thank you very much, professor. I'll take the make-up exam.\n",
            "\n",
            "\n",
            "\n",
            "Korean:\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L73DNP8qPKdy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
